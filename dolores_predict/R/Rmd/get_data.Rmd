---
title: "get_data_tests"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(RNRCS)
library(fable)
library(readr)
library(lubridate)
options(scipen = 999)
```

## Get snotel data
```{r}
meta_data<-grabNRCS.meta(ntwrks = c("SNTL", "SNTLT", "SCAN"))

dolores_sites<-meta_data[[1]]%>%
  as_tibble()%>%
  filter(state =="CO", str_detect(huc, "140300"))%>%
  mutate(site_id_num = as.numeric(str_match_all(site_id, "[0-9]+")))

dolores_site_ids<-dolores_sites%>%
  pull(site_id_num)%>%
  unlist()%>%
  as.numeric()
```

## Get snotel Sites
```{r}
data_test<-grabNRCS.data(network = "SNTL", 
              site_id = 1185, 
              timescale = "daily", 
              DayBgn = '1985-01-01',
              DayEnd = Sys.Date()
)%>%as_tibble()

get_snotl_data<-function(site_id){
  grabNRCS.data(network = "SNTL", 
              site_id = site_id, 
              timescale = "daily", 
              DayBgn = '1985-01-01',
              DayEnd = Sys.Date()
              )%>%
    as_tibble()%>%
    mutate(site_id_num = site_id)
}

all_sntl_data<-lapply(dolores_site_ids, get_snotl_data)%>%
  bind_rows()

se_site<-all_sntl_data%>%
  left_join(select(dolores_sites, site_id_num, site_id), by= "site_id_num")%>%
  select(Date, Snow.Depth..in..Start.of.Day.Values, Snow.Water.Equivalent..in..Start.of.Day.Values, site_id_num, site_id)%>%
  mutate(
    Date = as.Date(Date),
    Year = year(Date)
    )%>%
  rename(snow_depth = 2, snow_water_eq=3)%>%
  filter(site_id_num %in% c(465, 586, 589, 739))

se_site_yr<-se_site%>%
  group_by(year(Date), site_id_num)%>%
  summarize(max_se = max(snow_water_eq, na.rm = T))%>%
  ungroup()%>%
  rename(year=1)

avg_snwater_eq<-se_site_yr%>%
  filter(year>1986)%>%
  group_by(year)%>%
  summarize(avg_snow_water_e = mean(max_se))
  
  
```

## Attempting Prediction of Snowpack
```{r}
avg_sn<-se_site%>%
  group_by(Date)%>%
  summarize(avg_sn_eq =  mean(snow_water_eq, rm.na = T))

avg_sn%>%
  ggplot(aes(Date, avg_sn_eq))+
  geom_line()

avg_sn%>%
  as_tsibble(
    index = "Date",
    interval = "D"
  )

avg_sn_ts<-avg_sn%>%
  as_tsibble(index = Date)

fit<-avg_sn_ts%>%
  model(
    arima = ARIMA(avg_sn_eq~fourier("year", K = 20))
  )
fit%>%
  forecast(h = "6 months")%>%
  autoplot(avg_sn_ts%>%filter(year(Date)>2018))

pred_sn_pk<-fit%>%
  forecast(h = "6 months")%>%
  filter(avg_sn_eq == max(avg_sn_eq))%>%
  mutate(ci_90 = hilo(.distribution, 90))%>%
  mutate(lower = ci_90$.lower, upper = ci_90$.upper)%>%
  as_tibble()%>%
  select(Date, avg_sn_eq, lower, upper)%>%
  pivot_longer(-Date, names_to = "estimated_eq", values_to = "avg_snow_water_e")%>%
  mutate(year = year(Date))%>%
  select(-Date)

```


## BOR Data
```{r}
bor_meta<-grabNRCS.meta(ntwrks = c("BOR"))[[1]]%>%
  as_tibble()

bor_meta%>%
  filter(str_detect(county, "Montezuma"))%>%
  select(site_id, site_name)


bor_data<-grabBOR.data(site_id = "MPHC2000", 
                       timescale = 'daily', 
                       DayBgn = '1985-01-01',
                        DayEnd = Sys.Date())%>%
  as_tibble()%>%
  mutate(ds = as.Date(Date),
         y = as.numeric(`Reservoir Storage Volume (ac_ft) Start of Day Values`))%>%
  select(ds, y)

bor_data%>%
  tail()

bor_data%>%
  rename(volume = 2)%>%
  mutate(Date = as.Date(Date), volume = as.numeric(volume), year = year(Date))%>%
  ggplot(aes(yday(Date), volume, color = as.factor(year)))+
  geom_line()+
  theme_light()
```

# Downloading Flow data from USGS
```{r}
flow_data<-read_tsv("https://waterservices.usgs.gov/nwis/dv/?format=rdb&sites=09169500,%2009166500&startDT=1985-02-01&endDT=2020-02-07&statCd=00003&siteType=ST&siteStatus=all", skip = 35)%>%
  select(2:5)%>%
  rename(site_id = 1, date = 2, flow=3, code = 4)%>%
  mutate(site_id = ifelse(site_id == "09166500", "Dolores", "Bedrock"))%>%
  drop_na()

bedrock_flow<-flow_data%>%
  filter(site_id == "Bedrock"& year(date)>1986)

bedrock_flow%>%
  ggplot(aes(date, flow))+
  geom_line()

bedrock_flow%>%
  group_by(year(date), month(date))%>%
  filter(flow>1000)%>%
  summarize(max = max(flow), count = n())%>%
  ungroup()%>%
  filter(max>1000)%>%
  View()

predicted_variable<-bedrock_flow%>%
  filter(flow>1000 & month(date) %in% c(3:6))%>%
  count(year(date))%>%
  rename(year = 1, raftable_releases = 2)
```

## Get McPhee Data from NRCS API
```{r}
bor_data<-grabBOR.data(site_id = "MPHC2000", 
                       timescale = 'daily', 
                       DayBgn = '1985-01-01',
                        DayEnd = Sys.Date())%>%
  as_tibble()%>%
  mutate(date = as.Date(Date),
         res_volume = as.numeric(`Reservoir Storage Volume (ac_ft) Start of Day Values`))%>%
  select(date, res_volume)

bor_data%>%
  mutate(res_volume = ifelse(is.na(res_volume), lag(res_volume), res_volume))%>%
  filter(is.na(res_volume)) 

res_vol<-bor_data%>%
  filter(month(date) %in% c(12, 01, 02))%>%
  group_by(year(date))%>%
  summarize(avg_vol = min(res_volume, na.rm = T))%>%
  ungroup()%>%
  rename(year = 1)%>%
  filter(year>1986)
```


## Create Variable to predict for regression
```{r}
var_df<-predicted_variable%>%
  full_join(avg_snwater_eq)%>%
  full_join(res_vol)%>%
  mutate(raftable_releases = ifelse(is.na(raftable_releases), 0, raftable_releases))

var_df_train<-var_df%>%
  filter(year!=2020)

var_df%>%
  filter(avg_vol<240000)

var_df%>%
  ggplot(aes(raftable_releases, avg_snow_water_e, size = avg_vol))+
  geom_point()
```

## Random Forest Model
This didn't really work! Not enough data. 
```{r}
library(caret)
set.seed(1234)

control <- trainControl(method="cv", number=10, search = "random")

rf_default <- train(raftable_releases~avg_snow_water_e+avg_vol, 
                    data=var_df_train, 
                    method="rf", 
                    tuneLength=3,
                    ntree = 2000,
                    trControl=control)

rf_default
var_df_train%>%
  mutate(prediction = stats::predict(rf_default, .))%>%
  ggplot(aes(raftable_releases, prediction))+
  geom_point()+
  geom_text(aes(label=year),hjust=0, vjust=0)

pred_sn_pk_1<-pred_sn_pk%>%
  mutate(avg_vol = var_df%>%
           filter(year == 2020)%>%
           pull(avg_vol))

test_data<-var_df%>%
  filter(year==2020)%>%
  select(-raftable_releases)%>%
  mutate(estimated_eq = "current")%>%
  bind_rows(pred_sn_pk_1)

test_data%>%
  mutate(prediction = stats::predict(rf_default, .))

stats::predict(rf_default, var_df)

?predict()
```


## Linear Poisson Model
This worked better. 
```{r}
model_lm<-glm(raftable_releases~avg_snow_water_e+avg_vol, family = "poisson", data = var_df_train)

var_df_train%>%
  mutate(prediction = stats::predict(model_lm, .))%>%
  ggplot(aes(raftable_releases, prediction))+
  geom_point()+
  geom_text(aes(label=year),hjust=0, vjust=0)

model_lm%>%
  augment(data = var_df_train, type.predict = "response")%>%
  ggplot(aes(raftable_releases, .fitted))+
  geom_point()+
  geom_text(aes(label=year),hjust=0, vjust=0)

model_lm%>%
  augment(newdata = test_data, type.predict = "response")
```


## strait up linear model
This worked also, but in theory should not work as well because the predicted value is technically count data. 
```{r}
lm<-lm(raftable_releases~avg_snow_water_e+avg_vol, data = var_df_train)

lm%>%
  augment(data = var_df_train, type.predict = "response")%>%
  ggplot(aes(raftable_releases, .fitted))+
  geom_point()+
  geom_text(aes(label=year),hjust=0, vjust=0)

lm%>%
  augment(newdata = test_data)
```







